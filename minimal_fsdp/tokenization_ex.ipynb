{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from datasets import Dataset\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load data and tokenize it.  This could, in  principle, be done in the datasets __getitem__, but why add work to IO when we can make it so much faster?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_pretrained('gpt2') #using a prior trained tokenizer\n",
    "print(tokenizer.get_vocab_size())\n",
    "split = 'test'\n",
    "data = pd.read_csv(f'data/shakespeare/{split}.csv')\n",
    "tokenized = tokenizer.encode(data['text'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, simply change the shaping of the array to form input sequences, and save as an H5 file for later.  Since this is GPT, we dont really need labels or attention masks.  We do make the input 1-token longer than necessary so that we can have the input be `tokens[:-1]`  and the label be `tokens[1:]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 256)\n"
     ]
    }
   ],
   "source": [
    "# get tokens from the string (its just one long string...)\n",
    "input_ids = np.array(tokenized.ids)\n",
    "# Pre-set sequence length (aka block size)\n",
    "seq_len = 256\n",
    "nsamples = input_ids.size // (seq_len+1)\n",
    "input_ids = input_ids[:nsamples*seq_len].reshape((nsamples, seq_len))\n",
    "print(input_ids.shape)\n",
    "# save as h5 file for easy use later\n",
    "with h5py.File(f'data/shakespeare/{split}.h5', 'w') as f:\n",
    "    f.create_dataset('input_ids', data=input_ids, dtype=np.uint16) # need uint16 to accomodate 50K tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thing = np.random.randint(0, 50257, (10,))\n",
    "tokenizer.decode([198])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genomelm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
